[{"authors":["admin"],"categories":null,"content":"I am a data scientist at NYU Grossman School of Medicine, specializing in education technology and learning analytics. With over 5 years of expertise, I excel in leveraging data to enhance analytics, extract insights, and design solutions. My focus spans diverse domains, but currently I am in the medical education sector.\nMy current focus is on developing and implementing MLOps for deploying and monitoring machine learning pipelines for experimentation and research. For th epast few years I have integrated end-to-end pipelines from notebooks into production code bases for feature processing and inference to predict note quality, screen medical school applications, and monitor student performance.\nI am dedicated to delivering impactful actions that drive business metrics and inform leadership decisions. My foundation in applied statistics and quantitative modeling empowers me to collaborate effectively with cross-functional teams, offering insights into data collection, efficient processing, visualization, and automated reporting.\nMy commitment to problem-solving extends beyond education. Whether in healthcare or other fields, I thrive on utilizing data, statistics, and machine learning to tackle complex challenges.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://ilanreinstein.github.io/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"I am a data scientist at NYU Grossman School of Medicine, specializing in education technology and learning analytics. With over 5 years of expertise, I excel in leveraging data to enhance analytics, extract insights, and design solutions.","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026#34;Courses\u0026#34; url = \u0026#34;courses/\u0026#34; weight = 50 Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026#34;Docs\u0026#34; url = \u0026#34;docs/\u0026#34; weight = 50 Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://ilanreinstein.github.io/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"https://ilanreinstein.github.io/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"https://ilanreinstein.github.io/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":[],"content":"So this has been in the back of my mind for a couple of years now. It all started in 2021, after all those months of lockdown with little to do but work and learn new things. I noticed that Data Engineering was becoming a thing, the \u0026ldquo;new hot data job\u0026rdquo;, and my read on the room was that this was only going to grow. I was aware of how much companies were rushing to the ML wave without having proper data infrastructure so I thought it would be an opportunity to get ahead and compete for these roles. Finally, I stopped being interested in doing applied stats and research, and during those covid months I realized how interested and efficient I was at data transformation, no wonder I spent a lot of time cleaning data for stats and an ML.\nI started reading about the topic, learning about the tools and concepts that a data engineer should know. After a few weeks, I found Udacity\u0026rsquo;s Nanodegree on Data Engineering. I won\u0026rsquo;t go into a long review of the certification so here\u0026rsquo;s the TL;DR version: I found it very helpful and it was just what I needed at the time to organize some ideas in my head and focus on my next steps. I got hands-on exposure to tools and fired up a million more questions in my head.\nYears went by I was still unsatisfied with my learning and my growth. Not been able to break into the field after months of trying and reading content from experts with no practical experience gained was adding up to my frustration. It was one night though, while reading a book on basics of investing that I got an idea. Wouldn\u0026rsquo;t it be a cool project to build a simple dashboard that captures relevant metrics that a value investor uses? I saw this as a win-win. I could learn data engineering while at the same time gaining some confidence to start investing in the stock market.\nProject Goal So the goal of this project consists in finding a data source (preferably free API) and build a pipeline that feeds into an analytics-ready table.\nArchitecture or Setup Since this is first time, I wanted to keep things simple enough so I don\u0026rsquo;t get caught in the midst of troubleshooting or debugging tools. For scheduling and executing the pipelines I decided to go with Airflow. It seems to remain as the industry standard for many and has enough support and documentation online. I am running the standalone version of it on my local machine but I am using AWS for storage and eventually hosting the UI.\nFor dumping raw data and creating a staging layer I am using AWS S3. Again, no particular reason other than familiarity and the fact that I have done other projects on GCP Cloud Storage.\nETL or Pipeline   Data Source: There are many useful and rich APIs for financial data. Mostly focus on stock price and those higher frequency data and we\u0026rsquo;re interested in Financials or Fundamentals of a company. As you ay imagine there are a lot of paid APIs with super rich data, and those that are free impose rate limits. The winner for me isAlpha Vantage. The free tier does give me historical data but I am limited to 25 call per day; more on that later. As a starting point, I will only collect data for companies in the SP500. To vuild the dashboard I need to get Income Statement, Balance Sheet, Cash Flow and Earnings reports, four endpoints.\n  Given the limit on calls to the API, I took this not as a problem but an opportunity to work my way through a problem that may appear in the real world of Data Engineering. So in order to fill my \u0026ldquo;data warehouse\u0026rdquo; with financial statements from 500 companies the pipeline would have to respect and keep track of how many calls it has done per day. Since I need to hit four endpoints per company, I can only process six companies per day, therefore my pipeline will complete (if nothing breaks and I don\u0026rsquo;t spend all that time troubleshooting) in approximately 80 days, just in time for holidays.\n  For each company I dump each of the four raw JSONs into S3. I then load these and combined them into a single table (one per endpoint) and store this as a Parquet file in the Staging layer. Finally, and this is where I am right now, I compute the metrics of interest to present in the dashboard. This step requires a bit more time, as I know almost nothing about Value Investing, other than understanding a few key terms here and there. For data frames in Python I am using Polars. I am not as proficient as in Pandas but hey, this is a learning project and cleaning a table with another library/package only adds value to my journey.\n  Once I have the metrics, I will use DuckDB as a portable data warehouse with a clean table ready to use for visualization (TBD). I am planning on using Streamlit for the UI and putting it on AWS for sharing.\n  There are not particular reasons as to why I chose some of the technologies in my pipeline, I just spend a lot of time reading about what other Data Engineers do and what are common denominators for simple batch processes.\nChallenges  The constraint for API call limits pointed me to look at XCom variables in Airflow, something I never heard of before or don\u0026rsquo;t rememebr from my Udacity days. Trying to make these work has taken a lot of time and troubleshooting Setting up Airflow locally. I played around with different approaches and always aim to go the Docker route. However, I wanted to get the ball rolling quick due to my impatience to get my hands dirty and trying things out. Astronomer looked nice, but didn\u0026rsquo;t work for me (my fault most likely). The workflow of updating the script, parsing it again in Airflow, clearing tasks and running again is a bit cumbersome. I would love to hear what more advanced people do to tackle this, or what workflows are recommended to avoid this. Data Modeling. This one is the biggest stone in my shoe. I find data modeling one of the most interesting parts of this and learning about it hasn\u0026rsquo;t been a walk in the park. I have not read Kimball or Inmon, but I know the basic concepts from my previous bootcamp/nanodegree. For some reason though, thinking about it in practice always feels complicated. Will keep on practicing to get better. Cloud Computing. My experience has mostly been focused on internal servers so understanding what services will I need to deploy will be an interesting endeavor, considering that I will need to keep costs as low as possible.  Results (coming soon..) ","date":1728432000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728521636,"objectID":"771139c9bb1411120c7d7f77cc339426","permalink":"https://ilanreinstein.github.io/project/value-investing-and-data-engineering/","publishdate":"2024-10-09T00:00:00Z","relpermalink":"/project/value-investing-and-data-engineering/","section":"project","summary":"So this has been in the back of my mind for a couple of years now. It all started in 2021, after all those months of lockdown with little to do but work and learn new things.","tags":["data engineering"],"title":"Value Investing and Data Engineering","type":"project"},{"authors":[],"categories":[],"content":"Introduction In a previous post, I told you pretty much all my trajectory in the world of data science. For the next series of entries, I\u0026rsquo;d like to stop complaining and rambling and actually do get some work done to share with you and others.\nOne of the hardest things to move onto an engineering role is having the hands-on, practical experience and exposure to the concepts and workflows that are so natural or common to CS and Software folks. As much I try to come up with side additions to my projects at work, delivering results is a priority so there is little room to go deep on certain concepts, so I am at a constant false start on my learning. Luckily, I have established a rich list of people on LinkedIn whose content is very spot on and actually motivational.\nThis project started as a take-home test. I was contacted and decided to give it a shot to see where I am. You guessed right, total failure. However, the right failure at the right time. After realizing how far I was able to advance, I decided to build upon my submission and use what I see daily from experts on LinkedIn to build an actual working pipeline.\nProject Goals To build and expose an endpoint using a pre-trained model. The system should be able to write responses or predictions in a database and it should be packaged as a Docker container. Easy right? Well.. For someone whose experience is focused primarily on batch jobs that rely on a pristine data warehouse this was clearly a step up.\nLet me walk you through a couple of things I encountered along the way. If any of my takeaways, challenges, or next steps seem off, please do not hesitate to correct me or drop me a line to discuss.\nDeliverables This is a simple first iteration of a personal attempt at creating a ML system from the ground. These are th components:\n A Docker file to run the environment and fire up the API The code for feature processing and predictions A SQLite database that saves requests from the API. Only predictions are stored so far, next would be to keep a cache of \u0026ldquo;historical\u0026rdquo; features. Recall this is the Titanic competition turned into and MLOps project so more data is not available. Basic tests for the feature processing pipeline A failed attempt to draw a Cloud architecture capable of hosting such system  The API works perfect and the database interaction is smooth. As a local Docker container I was able to obtain predictions such that ther would be no duplicate entries. A fancier and more robust way for this database interaction to happen may be nice to have.\nChallenges  I am mostly familiar with batch jobs that run on premise as Docker containers. The hardest part was to think about the database interaction. I found it very challenging to try and separate the components of the API along with the database. However, thinking hard about this helped me solidify my understanding of OLAP vs OLTP. The model selected for deployment made it hard to think about scale since it is a static problem from which we cannot get more data for improvement of the model, only more complex feature engineering with the embedded risk of overfitting. Lack of familiarity with API deployment and proper testing of these. Not entirely sure about the proper tests required as part of an API, but feels like a low-hanging-fruit to implement in a next iteration. Understanding the Git Workflow, other than simply creating the branches to add pieces of the application. I could improve my knowledge about collaboration with distributed teams: feature branches that depend on each other, merging strategies, etc.  Takeaways I have become a firm believer that in order to learn something well, you need to face it and do it. Reading and watching tutorials will only get you so far. It is amazing how much I got out of this small project and how I can start to think about transitioning more into an MLOps role.\nIn my humble and ignorant opinion, this has nothing to do with ML at all. It took me a few days of working in this project to realize I was doing backend engineering but maybe I\u0026rsquo;m wrong. Yes, the model quality and the data preparation steps are critical to the overall deployment performance of an inference API, but there are so many more components that are not easily taught through classic textbook examples.\nIn addition, many of the evaluation and monitoring of a model are concepts that as far as my experience and knowledge goes, is also a recent topic of research and application. Yes, there are many tools available but as a person trying to grow and learn this only leads to vendor and option anxiety. More and more am I convinced about learning the fundamental concepts first to then go with the tool that suits the needs of the problem.\nNext Steps   I\u0026rsquo;d like to see how this plays out in a Cloud Environment. What are the tools for this? What is a common architecture? How would I handle the costs? I have deployed some Docker images for R Shiny applications in the past, but this is a different animal for which the resources available are a bit overwhelming.\n  I am eager to integrate better tests. Not just unit tests for features but a robust data quality testing strategy, integration tests, etc.\n  Implement monitoring of predictions. Again, hard to do in this snapshot data problem but may be a good exercise to try out what sort of metrics I would monitor and how.\n  Setup a CI/CD with Github actions. I have worked on GitLab for our internal projects and would like to understand how is different.\n  Add the Features to the SQLite database. Although not entirely called for in this small, static dataset project I would be very interested in learning how to engineer Feature Stores for when you need new data daily.\n  Find a topic of interest or another data set which could be potentially more dynamic to do retraining or model improvement via feature engineering once the infrastructure is setup. This right now seems miles away, I have so many questions on how to do this. Perhaps I should ask a Data Engineer ;)\n  Conclussion This last one could be bigger than just that. I\u0026rsquo;d love to be able to choose a problem of my interest and collect some data. But I have to confess, seems a but overwhelming. It is hard to choose a topic. There are many things am interested in but none for which collecting data seems doable. Yes, I could scrape or find APIs, but I am talking about my lack for subject matter expertise on these things so it is difficult to determine common or even relevant questions that a predictive model can solve.\nAnother thing is I don\u0026rsquo;t want to focus so much on models. There are a few things that have come to mind in terms of potential projects, eg. financial data. Currently I\u0026rsquo;m working on another personal project (more focused on Data Engineering) and there is a plenty of data with lots of possibilities. However, I know very little of Time Series and ML and Financial Models, etc. Never to late to learn but for the purpose of learning MLOps feels like a stretch.\nAfter working a few weeks on this I am happy to share this journey with you. As I get some free time on my hands I will continue to update and improve on this repo with all things I\u0026rsquo;d like to integrate. In the meantime, I am happy with the result (well, the API works). This is fun and as mentioned above, it is the best way for me to learn about something, trial and error.\n","date":1724716800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1724811484,"objectID":"da47a7578997fce404fa3c0852f9a5c0","permalink":"https://ilanreinstein.github.io/project/titanic-mlops/","publishdate":"2024-08-27T00:00:00Z","relpermalink":"/project/titanic-mlops/","section":"project","summary":"First milestone of my MLOps learning journey, I hope not to drown..","tags":["machine learning","MLOps"],"title":"Titanic MLOps","type":"project"},{"authors":[],"categories":[],"content":"Recently, I have been bashing my head against the wall as to where do I fit in the big world of data science. I consider myself a data scientist, and have been doing so for the most part of my career (7-ish years or so). But lately, ever since my title changed, my role and tasks are somewhere in a bizarre transition, or better, in an constantly-evolving void of multifaceted responsibilities which I struggle to put on a piece of paper, therefore making it hard to determine where to focus in my career development endeavors. This is in no way a rant or a complain, I simply thought it was a good moment to express my opinions of the current inventory of roles after a few years in the field.\nIn a previous post of this blog, I wrote about how I moved from a physics degree to data science in higher education. I started off as an Associate Research Scientist: a fancy title that helped me put my name and see my contributions on multiple peer-reviewed papers, something I am very proud of. A few years later, I got my promotion and a title change to something that was odd to me at first, but with time it has started to stick with me, the infamous Data Science Engineer role.\nWhat is a Data Science Engineer?\nI have been asked about this in multiple conversations with people in the data space, and I always end up feeling like I could have explained it a bit better.\nI have been trying to put together a clear and concise definition of this role as it is not a common role you see on job boards of companies or in social networks like LinkedIn. It is why in this entry I\u0026rsquo;d like to provide a bit more insight into the profile of a Data Science Engineer and share my experience to also shed a light to anyone interested. You can read this as a blatant self-marketing/promoting piece of writing, nonetheless, I do think it is important to help job seekers and companies alike understand that people like me exist and that there are no clear pathways in this \u0026ldquo;sexiest job of the 21st century BS\u0026rdquo;.\nWhat can a Data Science Engineer do?\nYou\u0026rsquo;ll hate this answer but, pretty much anything. In my particular case, coming from a physics background I think the biggest asset has been troubleshooting, researching, and trial and error. Stats were not something that came natural to me and I had to work longer hours to try and get the concepts in my head. To the day, I still need to review basic things to understand what is going on. From a software perspective, I never really learned Computer Science basics like Data Structures and Algorithms, Architecture, none of that. However I did pick up basic (really basic) concepts pretty quickly once I started to get exposed to them, and of course, I was hooked.\nAre you a data scientist, data analyst, Data Engineer, or a Machine Learning Engineer?\nNeither and all of them. On a daily basis, I can go from crafting some interesting visuals for a paper, poster, or dashboard. The next day I am monitoring our ML pipelines and fixing the features store or codebase in case there are errors on the batch run from the night before. Some afternoons I see myself opening a stats book trying to remember how to interpret a logistic regression, or how to perform propensity score matching for a specific ad-hoc request from a stakeholder. In my free time, I troubleshoot and develop R Shiny applications as a freelancer, and I do everything from making a button work to putting the app on AWS for publication and sharing.\nNow here\u0026rsquo;s the problem. A data scientist by paper is focused primarily on building cool models, training algorithms in a Jupyter notebook, or reading a recent paper on some new fancy neural network architecture, I am not that person, but have been time to time. A data analyst crafts dashboards and know metrics well enough to build engaging reports after arduous data transformation work done in SQL (mostly). A Machine Learning Engineer may come more frequently from the Software Development space, with a broader knowledge on architecture, infrastructure, and super focused on more than classic statistical learning to include deep learning concepts. Finally, a data engineer focuses on building data pipelines, data models, and allocating resources usually on the cloud. This kind of profile is the one you\u0026rsquo;d hired 10 years ago for \u0026ldquo;Big Data\u0026rdquo;.\nSo now, back to our original question: Who am I? Why am I here? I have been exposed to many of the above \u0026ldquo;job responsibilities\u0026rdquo; to a certain degree. My role has varied by project, by business need, and by employer. I would probably attribute this to the fact that I started in data when there was still no clear definition of roles. This I take as both an advantage and a liability. On the brighter side, I have developed a set of skills that allow me to refer to myself as a Swiss Army knife of data. I can talk to different people within my organization and understand the technical jargon, which then allows me to translate easily to non-technical stakeholders and other teams. The down side, I haven\u0026rsquo;t fully developed a single path so I can continue growing with the proper focus, nor have I been capable of promoting my contributions other than say\nAt first, as many junior data scientists, I was obsessed with advanced mathematics and their application to real-world problems. I have been lucky enough to work on a variety of projects involving complex concepts from psychometric theory and advanced statistics; the math has been there by my side and I have witnessed my impact on tangible problems. However, as time went by, I started to feel like many of my contributions were only limited to literature and academic discussions. I was slow at delivering value and outcomes because I was highly insecure about the rigor or sophistication of my work. It was at that point when I decided to focus on building stuff however imperfect, incomplete, buggy or raw it may be I just needed to put things out there.\nAfter a few months of prototyping multiple R Shiny apps and integrating cleaner code in my existing pipelines I decided to focus on moving away from the classic Data Scientist role onto a more engineering-heavy position. Many of the skills required to succeed as a ML or Data engineer come from having solid CS foundations, something a Physics degree or a data science position rarely will prepare you for. So, was this a smart move? Maybe not, as I knew I\u0026rsquo;d be climbing a steep learning curve. To be fair, my journey to become a data scientist was not short of challenges and rabbit holes, but at least the research and stats required to thrive was more \u0026ldquo;familiar\u0026rdquo; coming from physics.\nTo be continued \u0026hellip;.\n","date":1722643200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1722732266,"objectID":"c1fefb4e809f7d3e04c3a8457ec5f2cd","permalink":"https://ilanreinstein.github.io/post/on-data-science-roles/","publishdate":"2024-08-03T00:00:00Z","relpermalink":"/post/on-data-science-roles/","section":"post","summary":"Recently, I have been bashing my head against the wall as to where do I fit in the big world of data science. I consider myself a data scientist, and have been doing so for the most part of my career (7-ish years or so).","tags":[],"title":"On Data Science Roles...","type":"post"},{"authors":[],"categories":["projects"],"content":"This is another project that I recently added to my learning portfolio. Thanks to Darshil Parmar\u0026rsquo;s video for the inspiration and the skeleton to get this project up and running. For anyone interested in seeing Data Engineering projects in action, please follow Darshil for more content.\nWhat the project is about? The project consists in using a data sample of Uber rides to create a very simple star schema to do analytics on. Darshil breaks down each of the tasks very well so it is pretty easy to follow. In addition to showing how to build each of the data model\u0026rsquo;s components, he goes on to show how to setup a basic infrastructure on Google Cloud. For orchestration he uses Mage, which I had heard about but since I was first introduced to Airflow, I never dedicated much attention to it. I have to say, I really enjoyed working on it. At least for an \u0026ldquo;easy\u0026rdquo; task such as the one worked in the toy project, I got to see the potential of such tool and how nice it is to build pipelines.\nWhat I learned? Data Modeling The data model Darshil presents is very simple yet useful as a toy example for entry-level engineers like myself. I also did some modifications to the model given the way Polars handles the joins as opposed to Pandas, and also because I tried really hard to understand each step of the way in creating the Facts and Dimensions. For example, Darshil uses a main key in the original table which he then uses to join with all dimensions. This seemed to work in Pandas because of the keys in both tables are preserved after the join, somthing that does not happen in Polars. Therefore, I had to revisit previous bootcamp projects to understand how to perform the final join that produces the Fact Table.\nIf I understood correctly, I created surrogate keys for each dimension table and then I joined the resulting fact table not on these keys, but on the data fields. E.g., for Payment Type Code Dimension, I created the payment_type_id variable for each unique row, and then I combined with the Fact table using the actual payment_type_name, which is the only resulting column common to both Fact and Dim in this case. Again, not sure if this is the right way, but based on the way Polars handles the joins, I was left with this approach.\nAs a consequence, I had to eliminate Location Dimensions (Lat and Long) since I would have been joining on floats. These are the main differences with what Darshil presented. I was a bit confused for a moment, but I relied on previous projects to make some progress in the project.\nTools I got to interact with a few new tools which I had never used before like Mage. It was pretty straightforward to deploy and setup the UI, plus it takes care of adding some simple tests to your code, which is nice and instructional.\nI decided to add a twist to the whole project and take Polars for a test drive. This is a newer data manipulation package in Python which has been gaining popularity and traction among my LinkedIn connections, that is why I decided to try it out. It has very similar feel to PySpark, something I really enjoyed over the few hours I spent building the ETL. I can see why people are moving away from Pandas and going to Polars, it is way faster. I did not do any proper benchmark with big size data or any comparison in performance but it is cool to see a library performing that well.\nI was already familiar with GCP, but as everything in life, repetition and practice are key. Seeing another example at play and understanding what each component means was a great learning experience.\nNext up Getting these foundations for more complex projects is highly rewarding. It is only up to now that I found some bandwidth to sit down and try to be consistent with my journey to become a better data engineer.\nI like taking baby steps in the learning process, I find that doing this early on compounds quicker to larger projects and experience.\nI will now move this exact same pipeline to AWS, and instead of Mage I will revisit Airflow.\n","date":1695340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695432120,"objectID":"335b4ad71f788df454d75011012dc96c","permalink":"https://ilanreinstein.github.io/project/uber-data-modeling-project/","publishdate":"2023-09-22T00:00:00Z","relpermalink":"/project/uber-data-modeling-project/","section":"project","summary":"This is another project that I recently added to my learning portfolio. Thanks to Darshil Parmar\u0026rsquo;s video for the inspiration and the skeleton to get this project up and running. For anyone interested in seeing Data Engineering projects in action, please follow Darshil for more content.","tags":[],"title":"Uber Data Modeling Project","type":"project"},{"authors":[],"categories":[],"content":"Over the past few months, I have given multiple talks and participated in forums discussing my professional trajectory and my experience as a data scientist. In this post I will try to compile my path and give a few tips into how I think one may break into data science and analytics. This is not a novel guide, nor a comprehensive path to break into data science, but I think I\u0026rsquo;ve been exposed to enough to share my story.\nFrom physics to Data Science When I started my undergraduate degree in physics, I knew I always wanted to do hands-on problem solving. I was undecided on which branch of engineering I should choose to fulfill my career expectations. I thought physics would give enough technical abilities and a broad exposure to science and math to eventually drift towards a more clear path into problem solving in the real world. I was surprised and wrong with how things turned out..\nDuring my third and last year of college, I was already fantasizing going into grad school to pursue a PhD in some wacky quantum-focused specialty and problem. This process led me to a lot of frustration: exam preparations, applications, finding schools and potential advisors, you name it. I started to get the feeling this pathway was not very natural to my abilities and my aspirations, I felt like forcing me to do something I was not and that brought me a lot of pain as I felt increasingly under pressure.\nLuckily, after a few rejections I stopped to rethink my decisions, and I thought that maybe finding a job would be a more fulfilling approach to start growing and learning new skills. I started contacting my professors, applying to jobs, and connecting with people. To my surprise, just after a few weeks after graduating, I already had multiple interviews in different sectors. This process slowly but surely presented me with opportunities and potential paths to follow in my career as a problem solver. Over the multiple interviews, I heard words like Big Data, Data Science, Machine Learning, etc. I was thrilled. I thought this was an interesting opening worth diving into and it was then in 2014 when I managed to \u0026lsquo;break into data science\u0026rsquo;.\nEarly experiences ","date":1667347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667429527,"objectID":"14bc3cb8f1128a7f92585bc1fcb1b9d0","permalink":"https://ilanreinstein.github.io/post/my-journey-in-data-science/","publishdate":"2022-11-02T00:00:00Z","relpermalink":"/post/my-journey-in-data-science/","section":"post","summary":"Over the past few months, I have given multiple talks and participated in forums discussing my professional trajectory and my experience as a data scientist. In this post I will try to compile my path and give a few tips into how I think one may break into data science and analytics.","tags":["data science","career"],"title":"My Journey in Data Science...","type":"post"},{"authors":[],"categories":[],"content":"Residents receive infrequent feedback on their clinical reasoning (CR) documentation. While machine learning (ML) and natural language processing (NLP) have been used to assess CR documentation in standardized cases, no studies have described similar use in the clinical environment.\nThe authors developed and validated using Kane’s framework a ML model for automated assessment of CR documentation quality in residents’ admission notes.\n","date":1667347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667427440,"objectID":"d6daedab64281fcf6681cc57c60b1df8","permalink":"https://ilanreinstein.github.io/publication/notesense-clinical-documentation/","publishdate":"2022-11-02T00:00:00Z","relpermalink":"/publication/notesense-clinical-documentation/","section":"publication","summary":"Residents receive infrequent feedback on their clinical reasoning (CR) documentation. While machine learning (ML) and natural language processing (NLP) have been used to assess CR documentation in standardized cases, no studies have described similar use in the clinical environment.","tags":["machine learning","NLP","Medical Education"],"title":"NoteSense: Automated Assessment of Clinical Documentation","type":"publication"},{"authors":[],"categories":[],"content":"Residency programs face overwhelming numbers of residency applications, limiting holistic review. Artificial intelligence techniques have been proposed to address this challenge but have not been created. Here, a multidisciplinary team sought to develop and validate a machine learning (ML)-based decision support tool (DST) for residency applicant screening and review.\n","date":1631105941,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631105941,"objectID":"2841cfdfa3b87794dc0330ceeccb73cc","permalink":"https://ilanreinstein.github.io/publication/resident-retriever/","publishdate":"2021-09-08T07:59:01-05:00","relpermalink":"/publication/resident-retriever/","section":"publication","summary":"Residency programs face overwhelming numbers of residency applications, limiting holistic review.  Here, develop and validate a Machine Learning based decision support tool (DST) for residency applicant screening and review.","tags":[],"title":"Resident Retriever","type":"publication"},{"authors":[],"categories":[],"content":"This is one of the projects I am most proud of. It was a tremendously hard journey, with lots of obstacles, both technical and conceptual, which lead to a constant reformulation of our initial problem. Despite all the difficulties, we managed to build a relatively simple model which is to be tested on further studies and prototypes about learning curves. This is the project that got me started with Hierarchical Models and Bayesian Statistics. Even though we did not develop a full Bayesian model for this occasion, I was constantly learning new things about regression and multilevel models, which naturally takes you into the Bayesian world.\nI had the privilege to collaborate with experts in the field like Martin Pusic, Jennifer Hill, David Cook, and Matthew Lineberry, among others.\nIf you are interested in education, adaptive learning algorithms and the potential applications of multilevel models in this type of setting, then I\u0026rsquo;m sure this paper will be quite enjoyable.\nPlease do not hesitate to reach out with an questions you may have about this, I\u0026rsquo;ll be happy to discuss more.\n","date":1603238400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603322362,"objectID":"8093a920c5cd695716ffba53d62b3962","permalink":"https://ilanreinstein.github.io/publication/learning-curves-multilevel-models/","publishdate":"2020-10-21T00:00:00Z","relpermalink":"/publication/learning-curves-multilevel-models/","section":"publication","summary":"The developed approach can thus give researchers and educators a better basis on which to anticipate learners’ pathways and individually adapt their future learning.","tags":["multilevel models","learning curves"],"title":"Learning Curves and Multilevel Models","type":"publication"},{"authors":[],"categories":[],"content":"As one of many of NYU Langone\u0026rsquo;s data scientists, I got to collaborate with the wonderful team from the Predictive Analytics Unit from NYU\u0026rsquo;s Medical Center to develop a model capable of predicting which patients will have favorable outcomes after being hospitalized. These were intense weeks not only at NYU and I am proud of being able to contribute to this project and help our physicians during these challenging times.\n","date":1603238400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603323703,"objectID":"f35797c52e834923bbdda584eecacf0f","permalink":"https://ilanreinstein.github.io/publication/nyu-covid-19-response/","publishdate":"2020-10-21T00:00:00Z","relpermalink":"/publication/nyu-covid-19-response/","section":"publication","summary":"The COVID-19 pandemic has challenged front-line clinical decision-making, leading to numerous published prognostic tools. Here, we use retrospective and prospective hospitalizations to develop and validate a parsimonious model to identify patients with favorable outcomes, based on real-time lab values, vital signs, and oxygen support variables.","tags":[],"title":"NYU's COVID-19 Response","type":"publication"},{"authors":[],"categories":[],"content":"In this post, I summarize a series of resources to get started with Bayesian Statistics. I compiled these references myself based on my own experience and opinion as to what a good introduction and next steps are in this process. This is not an academic curriculum or anything tremendously rigorous, but it is a comprehensive list that will surely get you embarked on the journey to revisiting/starting your statistics. Many of the references below were recommended to me in several workshops I\u0026rsquo;ve attended and I wanted to share with those like me that want to be better at statistics and Machine Learning (ML).\nThe first resource I can think of out there for beginners interested in Bayesian statistics and modeling is Richard McElreath’s Statistical Rethinking. Here you’ll learn everything from applying Bayes’ rule in simple problems to complex multilevel/hierarchical models. Since this is not only a book but also a full course, you should definitely follow along your reading with the video lectures on YouTube. I would encourage you to work through the code and exercises at the end of each chapter, it is the best way to start getting hands on expertise in this topic. Another great thing is that the book’s R package rethinking and all the example code has been translated into multiple programming languages, so you are free to choose the one of your preference to go through the reading.\nA powerful reference for those that feel like brushing up your stats with real-world examples, Andrew Gelman and Jennifer Hill’s Data Analysis Using Regression and Multilevel/Hierarchical Models is a fantastic book for those with more interest on Applied Statistics on a social science setting. An updated version of this book is called Regression and Other Stories (Andrew Gelman, Jennifer Hill, Aki Vehtari) and it will be released later this year. If you\u0026rsquo;d like to work through another more advanced course on Bayesian Statistics, I suggest you visit Aki Vehtari’s teaching page. Although more challenging than McElreath\u0026rsquo;s class it is worth checking it out. Again, the course material is available in R and Python.\nNow, you’ve refreshed your basic stats and gone through most of McElreath’s book, but you are also looking into ML. You may wonder, where should I go? What to do next? Challenge yourself to read Christopher Bishop’s Pattern Recognition and Machine Learning. Bishop’s book presents all the well known ML concepts from regression and classification to neural networks through a concealed Bayesian lens. It is fair to say that I\u0026rsquo;ve deeply strengthen my knowledge in both ML and Bayesian statistics just by working through the first chapters. Although I highly recommend it, keep in mind this is an advanced book and it takes some time to get your head around the concepts.\nAt the same level as Bishop’s book, you can also find a rigorous and detailed explanation of Bayesian statistics and modeling on David MacKay’s Information Theory, Inference, and Learning Algorithms. I was fortunate enough to use this book in college, and it still proves to be essential whenever I want to revisit some probability concepts, Bayesian statistics and ML. The book\u0026rsquo;s webpage also provides software resources and examples for you to experiment and play around as you navigate the text.\nWhenever you feel comfortable enough with the basics to go deeper into the math and the theoretical concepts behind probabilistic modeling, probabilistic computation and applied Bayesian Statistics I strongly encourage you to read all of Michael Betancourt’s case studies in due order. This will be challenging to grasp at first, but as you go back over the content and dedicate the time to understand these concepts, the reward is huge.\nIt is not very often that you hear about the success stories of Bayesian methods in industry, specially nowadays against the huge amount of attention on ML and AI. For those that like podcasts to listen to stories about practitioners and people applying Bayesian statistics to their fields I strongly advise that you check out Alexandre Andorra\u0026rsquo;s podcast Learning Bayesian Statistics. Alex does an amazing job at interviewing professionals from a wide variety of fields, with incredibly diverse backgrounds and experiences, hence introducing you to the vast universe of applications of Bayesian statistics.\nOne final thing that it is a hard requirement and a common thread between the references and resources listed above is Markov Chain Monte Carlo (MCMC). In order to fully explore probability spaces and distributions you need efficient and reliable computational methods like MCMC and its advanced variants. Today, many programming languages are capable of implementing such advanced estimation algorithm but the most popular are 1) Stan, which is built on C++ and has multiple interfaces to R (rstanarm, brms), Python (PyStan), Julia and others; and 2) PyMC3. If you are interested in learning the basics you may visit their webpages to see examples with code.\nSome personal remarks on my own journey through Bayesian Statistics. This topic is not easy, you should invest some time to see some progress. The references above are a great starting point and I’ve noticed an important step forward in my learning path by trying to apply Bayesian methods to my own work. For me, its fundamentals are more intuitive and transparent, and overall simpler to grasp (at least conceptually), but considerably harder to apply.\n","date":1591660800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591747058,"objectID":"f7165b7106e6792eb54d332ee31cc846","permalink":"https://ilanreinstein.github.io/post/bayesian_resources/resources-to-learn-bayesian-stats/","publishdate":"2020-06-09T00:00:00Z","relpermalink":"/post/bayesian_resources/resources-to-learn-bayesian-stats/","section":"post","summary":"A comprehensive list of resources to embark in your Bayesian Stats journey.","tags":[],"title":"Resources to Learn Bayesian Stats","type":"post"},{"authors":[],"categories":["projects"],"content":"","date":1583971200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583971200,"objectID":"5707bb79fcf50b08379d2348835045c5","permalink":"https://ilanreinstein.github.io/project/kaggle-survey-2019/kaggle-survey-2019-latin-america-ml-community/","publishdate":"2020-03-12T00:00:00Z","relpermalink":"/project/kaggle-survey-2019/kaggle-survey-2019-latin-america-ml-community/","section":"project","summary":"A 3-day project tackling the Kaggle 2019 Survey data: My first take on a Kaggle, Stan and Ordinal Regression Models","tags":[],"title":"Kaggle Survey 2019 Analysis","type":"project"},{"authors":[],"categories":[],"content":"As part of my journey in Statistics and Machine Learning, I was fortunate enough to write original content about popular libraries, algorithms and software packages used in this line of work. This content was all developed during the time I worked with KDnuggets, a site that is widely recognized for its contributions on content about recent advances in the field of Big Data and Predictive Analytics.\nYou may visit the site and my contributions on this link.\nI hope you enjoy it!\n","date":1583884800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583972571,"objectID":"e920d8103a07cb2601e5e45d73810a48","permalink":"https://ilanreinstein.github.io/post/data-turbulence/content-machine-learning/","publishdate":"2020-03-11T00:00:00Z","relpermalink":"/post/data-turbulence/content-machine-learning/","section":"post","summary":"A series of tutorials on Machine Learning published on KDnuggets","tags":[],"title":"Content on Machine Learning","type":"post"},{"authors":[],"categories":[],"content":"As part of a consultancy project I did for the Interamerican Development Bank (IADB), I worked closely with the Energy and Oil \u0026amp; Gas division to develop a series of interactive tools that would allow them to summarize their data in a friendly way , and so that this information may be shared internally with other departments and externally with stakeholders, i.e., countries and governments.\nThe data used for this project was provided internally by the bank, as well as gathered from different sources like BP\u0026rsquo;s Annual Energy Report, OECD data, World Bank data, and OPEC data among others.\nThe actual product consists in a series of interactive maps developed in D3, and it is hosted inside the bank, therefore data and code may not be shared. However, an overview of the project may be observed in the following pictures.I hope you like my visualization work!\nCheers!\nEnergy Efficiency and Intensity   Gas Consumption Aggregated and General Information   Gas Consumption Diasggregated   Gas Import and Export Data   ","date":1518652800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580780657,"objectID":"64a99282757c100e8042e3d1576c132f","permalink":"https://ilanreinstein.github.io/post/iadb-energy-viz/latin-america-energy-market/","publishdate":"2018-02-15T00:00:00Z","relpermalink":"/post/iadb-energy-viz/latin-america-energy-market/","section":"post","summary":"The visual take on the energy market of Latin America, a project.'","tags":[],"title":"Latin America's Energy Market","type":"post"},{"authors":[],"categories":[],"content":"Finally, I have made some time to begin adding some interesting content to this blog. I\u0026rsquo;ve been looking for a few minutes to gather some of the music I\u0026rsquo;ve heard lately, and decide which one has been the most resonant with my daily mood. If you don\u0026rsquo;t like heavy metal, progressive music, or psychedelic rock, then maybe this site is not for you. If you do like it, then I will try to come up with great suggestions for you to listen during those times of tremendous focus. I am one of those people that needs to listen to some tunes in order to work faster and better, otherwise my routine will be dull, unexciting, and even worst, a field trip to my attention span. If sometimes you feel the same way, welcome to Resonance, where the musical recommendations are intended to resonate with your working day, in particular, while performing Data Science related tasks like data cleaning, testing, scripting, plotting, Kaggling, etc.\nI also hope this blog helps you and I explore new music for Data Science. I clearly intend to hear suggestions from others and apply them to whatever working mood I feel on a given day: your thoughts and ideas are welcome here. So, without further ado, here is the first entry of this blog. Please enjoy, rock on, and keep working.\n Today, I\u0026rsquo;ve been collecting information about Data Science and Analytics graduate programs around Europe. It has been a very mechanic, almost automatic activity, so I thought something structured, powerful, and continuous was necessary. That is why I tuned into Dream Theater, specifically, Six Degrees of Inner Turbulence. This album is from the early 2000\u0026rsquo;s and it is divided into two. I\u0026rsquo;ve had this album on my library for years now, but it was only until about a couple of weeks that I listened to it carefully while cruising the web and writing some code. I was blown away by the amount of work I managed to complete for the duration of the record.\nI kept playing it for next days while working and I was amazed by how much energy the album can transmit. The highly structured rhythm patterns and complex compositions are just amazing, as well as the diversity and variety of the songs, just enough variability to keep you interested and focused at the same time. One particular aspect of the album, which I think contributes enormously to my productivity, is the longevity and continuity of every song: a not uncommon feature in the band\u0026rsquo;s writing and playing style. The fact that you don\u0026rsquo;t notice when a song ends and the other begins is a key factor to stay focused as you hardly keep track of how much time has passed. That feeling, I believe is key when finding the right piece to sit down and get things done, an overwhelming sense of non-advancing, infinite time.\nListening to Portnoy\u0026rsquo;s double kick for hours is the best medicine for restlessness and constant distractions. If you manage to keep up with with his pace, you will most definitely maintain full concentration for long period of time, even at late-night hours. When is not the drums, someone else is going to be hitting those 16th notes at high speeds, and that for me is uplifting and motivating.\nAs you begin the album, you are greeted by The Glass Prison, an absolute 14-minute masterpiece, enough to get your to-do lists done at a nice rhythm. Then comes Blind Faith, usually this is the one that I need to listen to with full attention, I just enjoy that song too much. You will continue through this first part with great stamina and focus, and then\u0026hellip; Overture. A majestic instrumental piece which opens up the gates to what I think is the more conceptual part of the album, and from there on, the album has already absorbed you into its full depth. An interesting fact, if you listen to Dream Theater\u0026rsquo;s albums carefully, and in alphabetical order, for example, Scenes From a Memory -\u0026gt; Six Degrees, you will see that they are perfectly connected so you could listen to them for an entire day! However, we will not dive into that today so let\u0026rsquo;s return to our idea.\nBy the time I\u0026rsquo;ve finished the album, I have already crossed out more than half of my daily tasks and it\u0026rsquo;s not even lunch time. This album not just gives me push and stamina, but it is also loud and powerful enough to keep me focused and calm. On stressful days, listening to something heavy is always good, you don\u0026rsquo;t hear the noise of unnecessary worries and problems in your head, you manage to just keep moving and finish everything properly without being so restless. Needless to say, it\u0026rsquo;s Dream Theater we are talking about, so I had to stop every now and then to grasp and enjoy the epic guitar solos and the long, loud, and melancholic finales.\nI hope you listen and enjoy this incredibly epic, conceptual, fast-paced album. In a future post, I will write more about Dream Theater as it is my go-to band during working hours. They have tons of material, highly suited to do data science, so I will be circling back to them.\nRock n' roll\u0026hellip;\n","date":1512432000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580779555,"objectID":"820b1e5aaecbd978b1fdd724aea9fa52","permalink":"https://ilanreinstein.github.io/post/data-turbulence/six-degress-of-data-turbulence/","publishdate":"2017-12-05T00:00:00Z","relpermalink":"/post/data-turbulence/six-degress-of-data-turbulence/","section":"post","summary":"Looking for music for coding and data analysis? I have just the thing...","tags":[],"title":"Six Degress of Data Turbulence","type":"post"},{"authors":[],"categories":[],"content":"This post is simply to display and promote some of my early projects in analytics and data visualization. The topics are diverse, ranging from sports to energy economics. These are simply some exploratory and visual analyses I performed in 2014 when trying to promote our product. The purpose of these posts was to educate businesses on the potential of data-driven decisions as well as showcase and expose our data products and solutions.\nI am very fond of these small projects as it was my first experience using R and performing data analytics and visualization, thus encouraging me to pursue a career in data science. I hope you enjoy the read and the insight. I will try to produce newer posts in the near future.\nFollow this link to see the full Portfolio with both the code and the data story.\n","date":1396310400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580779727,"objectID":"40157f73098019a791f17dfe24ab57dd","permalink":"https://ilanreinstein.github.io/post/data-stories/my-first-data-stories/","publishdate":"2014-04-01T00:00:00Z","relpermalink":"/post/data-stories/my-first-data-stories/","section":"post","summary":"A series of exploratory data analysis posts from a decade ago.","tags":[],"title":"My first data stories...","type":"post"}]