<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Ilan Reinstein</title>
    <link>https://ilanreinstein.github.io/project/</link>
      <atom:link href="https://ilanreinstein.github.io/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 09 Oct 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://ilanreinstein.github.io/media/icon_hub1d11bc70e03781f784a7270814f1676_20092_512x512_fill_lanczos_center_3.png</url>
      <title>Projects</title>
      <link>https://ilanreinstein.github.io/project/</link>
    </image>
    
    <item>
      <title>Value Investing and Data Engineering</title>
      <link>https://ilanreinstein.github.io/project/value-investing-and-data-engineering/</link>
      <pubDate>Wed, 09 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://ilanreinstein.github.io/project/value-investing-and-data-engineering/</guid>
      <description>&lt;p&gt;So this has been in the back of my mind for a couple of years now. It all started in 2021, after all those months of lockdown with little to do but work and learn new things. I noticed that Data Engineering was becoming a thing, the &amp;ldquo;new hot data job&amp;rdquo;, and my read on the room was that this was only going to grow. I was aware of how much companies were rushing to the ML wave without having proper data infrastructure so I thought it would be an opportunity to get ahead and compete for these roles. Finally, I stopped being interested in doing applied stats and research, and during those covid months I realized how interested and efficient I was at data transformation, no wonder I spent a lot of time cleaning data for stats and an ML.&lt;/p&gt;
&lt;p&gt;I started reading about the topic, learning about the tools and concepts that a data engineer should know. After a few weeks, I found Udacity&amp;rsquo;s Nanodegree on Data Engineering. I won&amp;rsquo;t go into a long review of the certification so here&amp;rsquo;s the TL;DR version: I found it very helpful and it was just what I needed at the time to organize some ideas in my head and focus on my next steps. I got hands-on exposure to tools and fired up a million more questions in my head.&lt;/p&gt;
&lt;p&gt;Years went by I was still unsatisfied with my learning and my growth. Not been able to break into the field after months of trying and reading content from experts with no practical experience gained was adding up to my frustration. It was one night though, while reading a book on basics of investing that I got an idea. Wouldn&amp;rsquo;t it be a cool project to build a simple dashboard that captures relevant metrics that a value investor uses? I saw this as a win-win. I could learn data engineering while at the same time gaining some confidence to start investing in the stock market.&lt;/p&gt;
&lt;h2 id=&#34;project-goal&#34;&gt;Project Goal&lt;/h2&gt;
&lt;p&gt;So the goal of this project consists in finding a data source (preferably free API) and build a pipeline that feeds into an analytics-ready table.&lt;/p&gt;
&lt;h2 id=&#34;architecture-or-setup&#34;&gt;Architecture or Setup&lt;/h2&gt;
&lt;p&gt;Since this is first time, I wanted to keep things simple enough so I don&amp;rsquo;t get caught in the midst of troubleshooting or debugging tools.
For scheduling and executing the pipelines I decided to go with Airflow. It seems to remain as the industry standard for many and has enough support and documentation online. I am running the standalone version of it on my local machine but I am using AWS for storage and eventually hosting the UI.&lt;/p&gt;
&lt;p&gt;For dumping raw data and creating a staging layer I am using AWS S3. Again, no particular reason other than familiarity and the fact that I have done other projects on GCP Cloud Storage.&lt;/p&gt;
&lt;h2 id=&#34;etl-or-pipeline&#34;&gt;ETL or Pipeline&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Data Source: There are many useful and rich APIs for financial data. Mostly focus on stock price and those higher frequency data and we&amp;rsquo;re interested in Financials or Fundamentals of a company. As you ay imagine there are a lot of paid APIs with super rich data, and those that are free impose rate limits. The winner for me is&lt;a href=&#34;https://www.alphavantage.co&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Alpha Vantage&lt;/a&gt;. The free tier does give me historical data but I am limited to 25 call per day; more on that later. As a starting point, I will only collect data for companies in the SP500. To vuild the dashboard I need to get Income Statement, Balance Sheet, Cash Flow and Earnings reports, four endpoints.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Given the limit on calls to the API, I took this not as a problem but an opportunity to work my way through a problem that may appear in the real world of Data Engineering. So in order to fill my &amp;ldquo;data warehouse&amp;rdquo; with financial statements from 500 companies the pipeline would have to respect and keep track of how many calls it has done per day. Since I need to hit four endpoints per company, I can only process six companies per day, therefore my pipeline will complete (if nothing breaks and I don&amp;rsquo;t spend all that time troubleshooting) in approximately 80 days, just in time for holidays.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For each company I dump each of the four raw JSONs into S3. I then load these and combined them into a single table (one per endpoint) and store this as a Parquet file in the Staging layer. Finally, and this is where I am right now, I compute the metrics of interest to present in the dashboard. This step requires a bit more time, as I know almost nothing about Value Investing, other than understanding a few key terms here and there. For data frames in Python I am using Polars. I am not as proficient as in Pandas but hey, this is a learning project and cleaning a table with another library/package only adds value to my journey.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Once I have the metrics, I will use DuckDB as a portable data warehouse with a clean table ready to use for visualization (TBD). I am planning on using Streamlit for the UI and putting it on AWS for sharing.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are not particular reasons as to why I chose some of the technologies in my pipeline, I just spend a lot of time reading about what other Data Engineers do and what are common denominators for simple batch processes.&lt;/p&gt;
&lt;h2 id=&#34;challenges&#34;&gt;Challenges&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The constraint for API call limits pointed me to look at XCom variables in Airflow, something I never heard of before or don&amp;rsquo;t rememebr from my Udacity days. Trying to make these work has taken a lot of time and troubleshooting&lt;/li&gt;
&lt;li&gt;Setting up Airflow locally. I played around with different approaches and always aim to go the Docker route. However, I wanted to get the ball rolling quick due to my impatience to get my hands dirty and trying things out. Astronomer looked nice, but didn&amp;rsquo;t work for me (my fault most likely).&lt;/li&gt;
&lt;li&gt;The workflow of updating the script, parsing it again in Airflow, clearing tasks and running again is a bit cumbersome. I would love to hear what more advanced people do to tackle this, or what workflows are recommended to avoid this.&lt;/li&gt;
&lt;li&gt;Data Modeling. This one is the biggest stone in my shoe. I find data modeling one of the most interesting parts of this and learning about it hasn&amp;rsquo;t been a walk in the park. I have not read Kimball or Inmon, but I know the basic concepts from my previous bootcamp/nanodegree. For some reason though, thinking about it in practice always feels complicated. Will keep on practicing to get better.&lt;/li&gt;
&lt;li&gt;Cloud Computing. My experience has mostly been focused on internal servers so understanding what services will I need to deploy will be an interesting endeavor, considering that I will need to keep costs as low as possible.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;results-coming-soon&#34;&gt;Results (coming soon..)&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>Titanic MLOps</title>
      <link>https://ilanreinstein.github.io/project/titanic-mlops/</link>
      <pubDate>Tue, 27 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://ilanreinstein.github.io/project/titanic-mlops/</guid>
      <description>&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;In a previous post, I told you pretty much all my trajectory in the world of data science. For the next series of entries, I&amp;rsquo;d like to stop complaining and rambling and actually do get some work done to share with you and others.&lt;/p&gt;
&lt;p&gt;One of the hardest things to move onto an engineering role is having the hands-on, practical experience and exposure to the concepts and workflows that are so natural or common to CS and Software folks. As much I try to come up with side additions to my projects at work, delivering results is a priority so there is little room to go deep on certain concepts, so I am at a constant false start on my learning. Luckily, I have established a rich list of people on LinkedIn whose content is very spot on and actually motivational.&lt;/p&gt;
&lt;p&gt;This project started as a take-home test. I was contacted and decided to give it a shot to see where I am. You guessed right, total failure. However, the right failure at the right time. After realizing how far I was able to advance, I decided to build upon my submission and use what I see daily from experts on LinkedIn to build an actual working pipeline.&lt;/p&gt;
&lt;h2 id=&#34;project-goals&#34;&gt;Project Goals&lt;/h2&gt;
&lt;p&gt;To build and expose an endpoint using a pre-trained model. The system should be able to write responses or predictions in a database and it should be packaged as a Docker container. Easy right? Well.. For someone whose experience is focused primarily on batch jobs that rely on a pristine data warehouse this was clearly a step up.&lt;/p&gt;
&lt;p&gt;Let me walk you through a couple of things I encountered along the way. If any of my takeaways, challenges, or next steps seem off, please do not hesitate to correct me or drop me a line to discuss.&lt;/p&gt;
&lt;h2 id=&#34;deliverables&#34;&gt;Deliverables&lt;/h2&gt;
&lt;p&gt;This is a simple first iteration of a personal attempt at creating a ML system from the ground. These are th components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A Docker file to run the environment and fire up the API&lt;/li&gt;
&lt;li&gt;The code for feature processing and predictions&lt;/li&gt;
&lt;li&gt;A SQLite database that saves requests from the API. Only predictions are stored so far, next would be to keep a cache of &amp;ldquo;historical&amp;rdquo; features. Recall this is the Titanic competition turned into and MLOps project so more data is not available.&lt;/li&gt;
&lt;li&gt;Basic tests for the feature processing pipeline&lt;/li&gt;
&lt;li&gt;A failed attempt to draw a Cloud architecture capable of hosting such system&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The API works perfect and the database interaction is smooth. As a local Docker container I was able to obtain predictions such that ther would be no duplicate entries. A fancier and more robust way for this database interaction to happen may be nice to have.&lt;/p&gt;
&lt;h2 id=&#34;challenges&#34;&gt;Challenges&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;I am mostly familiar with batch jobs that run on premise as Docker containers.&lt;/li&gt;
&lt;li&gt;The hardest part was to think about the database interaction. I found it very challenging to try and separate the components of the API along with the database. However, thinking hard about this helped me solidify my understanding of OLAP vs OLTP.&lt;/li&gt;
&lt;li&gt;The model selected for deployment made it hard to think about scale since it is a static problem from which we cannot get more data for improvement of the model, only more complex feature engineering with the embedded risk of overfitting.&lt;/li&gt;
&lt;li&gt;Lack of familiarity with API deployment and proper testing of these. Not entirely sure about the proper tests required as part of an API, but feels like a low-hanging-fruit to implement in a next iteration.&lt;/li&gt;
&lt;li&gt;Understanding the Git Workflow, other than simply creating the branches to add pieces of the application. I could improve my knowledge about collaboration with distributed teams: feature branches that depend on each other, merging strategies, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;takeaways&#34;&gt;Takeaways&lt;/h2&gt;
&lt;p&gt;I have become a firm believer that in order to learn something well, you need to face it and do it. Reading and watching tutorials will only get you so far. It is amazing how much I got out of this small project and how I can start to think about transitioning more into an MLOps role.&lt;/p&gt;
&lt;p&gt;In my humble and ignorant opinion, this has nothing to do with ML at all. It took me a few days of working in this project to realize I was doing backend engineering but maybe I&amp;rsquo;m wrong. Yes, the model quality and the data preparation steps are critical to the overall deployment performance of an inference API, but there are so many more components that are not easily taught through classic textbook examples.&lt;/p&gt;
&lt;p&gt;In addition, many of the evaluation and monitoring of a model are concepts that as far as my experience and knowledge goes, is also a recent topic of research and application. Yes, there are many tools available but as a person trying to grow and learn this only leads to vendor and option anxiety. More and more am I convinced about learning the fundamental concepts first to then go with the tool that suits the needs of the problem.&lt;/p&gt;
&lt;h2 id=&#34;next-steps&#34;&gt;Next Steps&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;I&amp;rsquo;d like to see how this plays out in a Cloud Environment. What are the tools for this? What is a common architecture? How would I handle the costs?
I have deployed some Docker images for R Shiny applications in the past, but this is a different animal for which the resources available are a bit overwhelming.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I am eager to integrate better tests. Not just unit tests for features but a robust data quality testing strategy, integration tests, etc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Implement monitoring of predictions. Again, hard to do in this snapshot data problem but may be a good exercise to try out what sort of metrics I would monitor and how.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Setup a CI/CD with Github actions. I have worked on GitLab for our internal projects and would like to understand how is different.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add the Features to the SQLite database. Although not entirely called for in this small, static dataset project I would be very interested in learning how to engineer Feature Stores for when you need new data daily.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Find a topic of interest or another data set which could be potentially more dynamic to do retraining or model improvement via feature engineering once the infrastructure is setup. This right now seems miles away, I have so many questions on how to do this. Perhaps I should ask a Data Engineer ;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;conclussion&#34;&gt;Conclussion&lt;/h2&gt;
&lt;p&gt;This last one could be bigger than just that. I&amp;rsquo;d love to be able to choose a problem of my interest and collect some data. But I have to confess, seems a but overwhelming. It is hard to choose a topic. There are many things am interested in but none for which collecting data seems doable. Yes, I could scrape or find APIs, but I am talking about my lack for subject matter expertise on these things so it is difficult to determine common or even relevant questions that a predictive model can solve.&lt;/p&gt;
&lt;p&gt;Another thing is I don&amp;rsquo;t want to focus so much on models. There are a few things that have come to mind in terms of potential projects, eg. financial data. Currently I&amp;rsquo;m working on another personal project (more focused on Data Engineering) and there is a plenty of data with lots of possibilities. However, I know very little of Time Series and ML and Financial Models, etc. Never to late to learn but for the purpose of learning MLOps feels like a stretch.&lt;/p&gt;
&lt;p&gt;After working a few weeks on this I am happy to share this journey with you. As I get some free time on my hands I will continue to update and improve on this repo with all things I&amp;rsquo;d like to integrate. In the meantime, I am happy with the result (well, the API works). This is fun and as mentioned above, it is the best way for me to learn about something, trial and error.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Uber Data Modeling Project</title>
      <link>https://ilanreinstein.github.io/project/uber-data-modeling-project/</link>
      <pubDate>Fri, 22 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://ilanreinstein.github.io/project/uber-data-modeling-project/</guid>
      <description>&lt;p&gt;This is another project that I recently added to my learning portfolio. Thanks to Darshil Parmar&amp;rsquo;s video for the inspiration and the skeleton to get this project up and running. For anyone interested in seeing Data Engineering projects in action, please follow Darshil for more content.&lt;/p&gt;
&lt;h2 id=&#34;what-the-project-is-about&#34;&gt;What the project is about?&lt;/h2&gt;
&lt;p&gt;The project consists in using a data sample of Uber rides to create a very simple star schema to do analytics on. Darshil breaks down each of the tasks very well so it is pretty easy to follow. In addition to showing how to build each of the data model&amp;rsquo;s components, he goes on to show how to setup a basic infrastructure on Google Cloud. For orchestration he uses Mage, which I had heard about but since I was first introduced to Airflow, I never dedicated much attention to it. I have to say, I really enjoyed working on it. At least for an &amp;ldquo;easy&amp;rdquo; task such as the one worked in the toy project, I got to see the potential of such tool and how nice it is to build pipelines.&lt;/p&gt;
&lt;h2 id=&#34;what-i-learned&#34;&gt;What I learned?&lt;/h2&gt;
&lt;h3 id=&#34;data-modeling&#34;&gt;Data Modeling&lt;/h3&gt;
&lt;p&gt;The data model Darshil presents is very simple yet useful as a toy example for entry-level engineers like myself. I also did some modifications to the model given the way Polars handles the joins as opposed to Pandas, and also because I tried really hard to understand each step of the way in creating the Facts and Dimensions. For example, Darshil uses a main  key in the original table which he then uses to join with all dimensions. This seemed to work in Pandas because of the keys in both tables are preserved after the join, somthing that does not happen in Polars. Therefore, I had to revisit previous bootcamp projects to understand how to perform the final join that produces the Fact Table.&lt;/p&gt;
&lt;p&gt;If I understood correctly, I created surrogate keys for each dimension table and then I joined the resulting fact table not on these keys, but on the data fields. E.g., for Payment Type Code Dimension, I created the &lt;code&gt;payment_type_id&lt;/code&gt; variable for each unique row, and then I combined with the Fact table using the actual &lt;code&gt;payment_type_name&lt;/code&gt;, which is the only resulting column common to both Fact and Dim in this case. Again, not sure if this is the right way, but based on the way Polars handles the joins, I was left with this approach.&lt;/p&gt;
&lt;p&gt;As a consequence, I had to eliminate Location Dimensions (Lat and Long) since I would have been joining on floats. These are the main differences with what Darshil presented. I was a bit confused for a moment, but I relied on previous projects to make some progress in the project.&lt;/p&gt;
&lt;h3 id=&#34;tools&#34;&gt;Tools&lt;/h3&gt;
&lt;p&gt;I got to interact with a few new tools which I had never used before like Mage. It was pretty straightforward to deploy and setup the UI, plus it takes care of adding some simple tests to your code, which is nice and instructional.&lt;/p&gt;
&lt;p&gt;I decided to add a twist to the whole project and take Polars for a test drive. This is a newer data manipulation package in Python which has been gaining popularity and traction among my LinkedIn connections, that is why I decided to try it out. It has very similar feel to PySpark, something I really enjoyed over the few hours I spent building the ETL. I can see why people are moving away from Pandas and going to Polars, it is way faster. I did not do any proper benchmark with big size data or any comparison in performance but it is cool to see a library performing that well.&lt;/p&gt;
&lt;p&gt;I was already familiar with GCP, but as everything in life, repetition and practice are key. Seeing another example at play and understanding what each component means was a great learning experience.&lt;/p&gt;
&lt;h2 id=&#34;next-up&#34;&gt;Next up&lt;/h2&gt;
&lt;p&gt;Getting these foundations for more complex projects is highly rewarding. It is only up to now that I found some bandwidth to sit down and try to be consistent with my journey to become a better data engineer.&lt;/p&gt;
&lt;p&gt;I like taking baby steps in the learning process, I find that doing this early on compounds quicker to larger projects and experience.&lt;/p&gt;
&lt;p&gt;I will now move this exact same pipeline to AWS, and instead of Mage I will revisit Airflow.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kaggle Survey 2019 Analysis</title>
      <link>https://ilanreinstein.github.io/project/kaggle-survey-2019/kaggle-survey-2019-latin-america-ml-community/</link>
      <pubDate>Thu, 12 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://ilanreinstein.github.io/project/kaggle-survey-2019/kaggle-survey-2019-latin-america-ml-community/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
